{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSVEP: Offline processing using Machine Leaning Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Import neceessary toolboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import require library for preprocess\n",
    "import mne\n",
    "import numpy as np\n",
    "from mne.channels import make_standard_montage\n",
    "import matplotlib.pyplot as plt\n",
    "from mne.datasets import eegbci\n",
    "import scipy\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "\n",
    "# import require library for classification\n",
    "from sklearn.svm import SVC # SVM library\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis # LDA library\n",
    "from sklearn.neighbors import KNeighborsClassifier # KNN library\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix # Result representation\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_feature = \"fft\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Read data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read biosemi file (bdf)\n",
    "raw1 = mne.io.read_raw_bdf(\"C:\\\\Users\\\\pipo_\\\\OneDrive\\\\Desktop\\\\neuromedia\\\\group1_block1.bdf\", preload=True, verbose=False) \n",
    "raw2 = mne.io.read_raw_bdf(\"C:\\\\Users\\\\pipo_\\\\OneDrive\\\\Desktop\\\\neuromedia\\\\group1_block2.bdf\", preload=True, verbose=False) \n",
    "raw3 = mne.io.read_raw_bdf(\"C:\\\\Users\\\\pipo_\\\\OneDrive\\\\Desktop\\\\neuromedia\\\\group1_block3.bdf\", preload=True, verbose=False) \n",
    "raw4 = mne.io.read_raw_bdf(\"C:\\\\Users\\\\pipo_\\\\OneDrive\\\\Desktop\\\\neuromedia\\\\group1_block4.bdf\", preload=True, verbose=False) \n",
    "raw = mne.concatenate_raws([raw1, raw2, raw3, raw4])\n",
    "# eegbci.standardize(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data preprocessing -- set channel locations/ downsampling/ frequency filtering (bandpass)/ epoching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import filtfilt\n",
    "from scipy import signal\n",
    "\n",
    "# Set channel location\n",
    "montage = make_standard_montage(\"biosemi64\")\n",
    "raw.set_montage(montage, on_missing='ignore')\n",
    "\n",
    "# Downsample data (from 1024 to 512Hz) to save storage space \n",
    "raw = raw.resample(512, verbose = False)\n",
    "\n",
    "# Get events and timestamps\n",
    "events = mne.find_events(raw, shortest_event = 0, verbose = False) \n",
    "\n",
    "# Create event dictionary \n",
    "event_dict =  {'12Hz': 8,\n",
    "'24Hz': 4,\n",
    "'6Hz': 10,\n",
    "'30Hz': 2\n",
    "}\n",
    "\n",
    "# Use events and event dictionary to cut data into Epochs\n",
    "# ssvep_chans = ['O1','Oz','PO3','PO4','POz','O2']  # Reject O2 becuase noisy channel\n",
    "ssvep_chans = ['O1','Oz','O2']  \n",
    "\n",
    "Epochs = mne.Epochs(raw, events, \n",
    "    tmin= 0.0,  \n",
    "    tmax= 4.0,    \n",
    "    event_id=event_dict,\n",
    "    picks = ssvep_chans,\n",
    "    preload = True,\n",
    "    event_repeated='drop',\n",
    "    baseline= None,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "Epochs = Epochs.copy().crop(tmin = 0.0, tmax = 4.0)\n",
    "\n",
    "train_label = Epochs['12Hz','6Hz', '24Hz', '30Hz'].events[:,-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply Scipy Filter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pipo_\\AppData\\Local\\Temp\\ipykernel_19368\\1196626325.py:15: FutureWarning: The current default of copy=False will change to copy=True in 1.7. Set the value of copy explicitly to avoid this warning\n",
      "  Epochs_data = butter_bandpass_filter(Epochs.get_data(), lowcut = 2, highcut= 40, axis = 2)\n"
     ]
    }
   ],
   "source": [
    "from scipy import signal\n",
    "\n",
    "def butter_bandpass(lowcut,highcut,fs,order):\n",
    "    nyq = 0.5*fs\n",
    "    low = lowcut/nyq\n",
    "    high = highcut/nyq\n",
    "    b,a = signal.butter(order,[low,high],'bandpass')\n",
    "    return b,a\n",
    "\n",
    "def butter_bandpass_filter(data,lowcut = 6, highcut = 30, order = 4, axis = 1):\n",
    "    b,a = butter_bandpass(lowcut,highcut,512,order)\n",
    "    y = signal.filtfilt(b,a,data,axis=axis)\n",
    "    return y\n",
    "\n",
    "Epochs_data = butter_bandpass_filter(Epochs.get_data(), lowcut = 2, highcut= 40, axis = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step3: Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Fast Fourier Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(192, 3, 2048)\n",
      "(192, 6144)\n"
     ]
    }
   ],
   "source": [
    "# Compute FFT for each epoch and return the power spectral density\n",
    "def compute_fft(epoch_data, sampling_rate):\n",
    "\n",
    "    num_epochs, num_channels, num_timepoints = epoch_data.shape\n",
    "\n",
    "    freqs = np.fft.fftfreq(num_timepoints-1, 1 / sampling_rate)\n",
    "    \n",
    "    fft_data = np.zeros((num_epochs, num_channels, len(freqs)))\n",
    "\n",
    "    # Compute FFT for each channel and each epoch\n",
    "    for epoch_idx in range(num_epochs):\n",
    "        for ch_idx in range(num_channels):     \n",
    "            fft_result = scipy.fft.fft(epoch_data[epoch_idx, ch_idx, 0:2048])\n",
    "\n",
    "            power_spectrum = np.abs(fft_result) ** 2  # Power = |FFT|^2\n",
    "            fft_data[epoch_idx, ch_idx, :] = power_spectrum\n",
    "\n",
    "    return fft_data, freqs\n",
    "\n",
    "# Example usage\n",
    "fft_out, freqs_out = compute_fft(Epochs_data, 512)\n",
    "print(np.shape(fft_out))\n",
    "\n",
    "fft_train = np.stack([arr.flatten() for arr in fft_out])\n",
    "print(fft_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Power Spectrum Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "(192, 3, 152)\n",
      "(192, 456)\n"
     ]
    }
   ],
   "source": [
    "psd_epoch = Epochs['12Hz','6Hz', '24Hz', '30Hz'].pick(ssvep_chans).compute_psd(fmin=2.0, fmax=40.0)\n",
    "print(psd_epoch.shape)\n",
    "\n",
    "psd_train = np.stack([arr.flatten() for arr in psd_epoch])\n",
    "print(psd_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('datasets/SSVEP_test_epochs.pkl', 'rb') as f:\n",
    "#     test_epochs = pickle.load(f)\n",
    "\n",
    "# test_epochs = test_epochs.copy().crop(tmin = 0.0, tmax = 4.0)\n",
    "\n",
    "# fft_out, freqs_out = compute_fft(test_epochs['12Hz','6Hz', '24Hz', '30Hz'].pick(ssvep_chans).get_data(), 512)\n",
    "# fft_test = np.stack([arr.flatten() for arr in fft_out])\n",
    "\n",
    "# psd_epoch = test_epochs['12Hz','6Hz', '24Hz', '30Hz'].pick(ssvep_chans).compute_psd(fmin=1.0, fmax=40.0)\n",
    "# psd_test = np.stack([arr.flatten() for arr in psd_epoch])\n",
    "\n",
    "# mapping = {2: 8, 4: 4, 8: 10, 10: 2}\n",
    "# test_label = np.vectorize(mapping.get)(test_epochs['12Hz','6Hz', '24Hz', '30Hz'].events[:,-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step4: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Select Feature as train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "if select_feature == \"fft\":\n",
    "    x_train = fft_train\n",
    "\n",
    "elif select_feature == 'psd':\n",
    "    x_train = psd_train\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train, train_label, test_size=0.3, random_state=SEED)\n",
    "\n",
    "### Apply Scaler\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "\n",
    "scaler = RobustScaler()\n",
    "x_train_sc = scaler.fit_transform(x_train)\n",
    "\n",
    "with open(\"trained_model/Scaler.pkl\", \"wb\") as file:\n",
    "    pickle.dump(scaler, file)\n",
    "\n",
    "x_test_sc = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47473451434325803"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_sc.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5440335311439375"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_sc.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetConfusionMatrix(models, X_train, X_test, y_train, y_test, target_names):\n",
    "    y_pred = models.predict(X_train)\n",
    "    print(\"Classification TRAIN DATA \\n=======================\")\n",
    "    print(classification_report(y_true= y_train, y_pred=y_pred, target_names= target_names))\n",
    "    print(\"Confusion matrix \\n=======================\")\n",
    "    print(confusion_matrix(y_true= y_train, y_pred=y_pred))\n",
    "\n",
    "    y_pred = models.predict(X_test)\n",
    "    print(\"Classification TEST DATA \\n=======================\")\n",
    "    print(classification_report(y_true=y_test, y_pred=y_pred, target_names= target_names))\n",
    "    print(\"Confusion matrix \\n=======================\")\n",
    "    print(confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'solver': 'svd'}\n",
      "Best cross-validation score: 0.738\n",
      "Classification TRAIN DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        12Hz       0.72      0.74      0.73        35\n",
      "         6Hz       0.86      0.83      0.84        29\n",
      "        24Hz       0.88      0.80      0.84        35\n",
      "        30Hz       0.79      0.86      0.82        35\n",
      "\n",
      "    accuracy                           0.81       134\n",
      "   macro avg       0.81      0.81      0.81       134\n",
      "weighted avg       0.81      0.81      0.81       134\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[26  2  3  4]\n",
      " [ 4 24  0  1]\n",
      " [ 3  1 28  3]\n",
      " [ 3  1  1 30]]\n",
      "Classification TEST DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        12Hz       0.79      0.85      0.81        13\n",
      "         6Hz       0.84      0.84      0.84        19\n",
      "        24Hz       1.00      0.62      0.76        13\n",
      "        30Hz       0.65      0.85      0.73        13\n",
      "\n",
      "    accuracy                           0.79        58\n",
      "   macro avg       0.82      0.79      0.79        58\n",
      "weighted avg       0.82      0.79      0.79        58\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[11  1  0  1]\n",
      " [ 0 16  0  3]\n",
      " [ 2  1  8  2]\n",
      " [ 1  1  0 11]]\n"
     ]
    }
   ],
   "source": [
    "lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "param_grid = {\n",
    "    'solver': ['svd']\n",
    "}\n",
    "\n",
    "cv_splitter = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "tuned_clf_lda = GridSearchCV(estimator=lda, param_grid=param_grid,\n",
    "                    scoring='accuracy', refit='accuracy', cv=cv_splitter)\n",
    "\n",
    "tuned_clf_lda.fit(x_train, y_train)\n",
    "print(f\"Best parameters: {tuned_clf_lda.best_params_}\")\n",
    "print(f\"Best cross-validation score: {tuned_clf_lda.best_score_:.3f}\")\n",
    "label_names = ['12Hz', '6Hz', '24Hz', '30Hz']\n",
    "\n",
    "with open(\"trained_model/LDA_model.pkl\", \"wb\") as file:\n",
    "    pickle.dump(tuned_clf_lda, file)\n",
    "\n",
    "GetConfusionMatrix(tuned_clf_lda, x_train, x_test, y_train, y_test, label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'C': 10, 'kernel': 'rbf'}\n",
      "Best cross-validation score: 0.687\n",
      "Classification TRAIN DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        12Hz       1.00      1.00      1.00        35\n",
      "         6Hz       1.00      1.00      1.00        29\n",
      "        24Hz       1.00      1.00      1.00        35\n",
      "        30Hz       1.00      1.00      1.00        35\n",
      "\n",
      "    accuracy                           1.00       134\n",
      "   macro avg       1.00      1.00      1.00       134\n",
      "weighted avg       1.00      1.00      1.00       134\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[35  0  0  0]\n",
      " [ 0 29  0  0]\n",
      " [ 0  0 35  0]\n",
      " [ 0  0  0 35]]\n",
      "Classification TEST DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        12Hz       0.77      0.77      0.77        13\n",
      "         6Hz       0.82      0.74      0.78        19\n",
      "        24Hz       0.39      0.54      0.45        13\n",
      "        30Hz       0.80      0.62      0.70        13\n",
      "\n",
      "    accuracy                           0.67        58\n",
      "   macro avg       0.70      0.66      0.67        58\n",
      "weighted avg       0.71      0.67      0.68        58\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[10  0  3  0]\n",
      " [ 1 14  4  0]\n",
      " [ 2  2  7  2]\n",
      " [ 0  1  4  8]]\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'C':  [10, 100],\n",
    "    'kernel': ['rbf', 'poly']\n",
    "}\n",
    "\n",
    "svm_model =  SVC(random_state=SEED)\n",
    "cv_splitter = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "tuned_clf_svm = GridSearchCV(estimator=svm_model, param_grid=param_grid,\n",
    "                    scoring='accuracy', refit='accuracy', cv=cv_splitter)\n",
    "\n",
    "tuned_clf_svm.fit(x_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {tuned_clf_svm.best_params_}\")\n",
    "print(f\"Best cross-validation score: {tuned_clf_svm.best_score_:.3f}\")\n",
    "label_names = ['12Hz', '6Hz', '24Hz', '30Hz']\n",
    "\n",
    "with open(\"trained_model/SVM_model.pkl\", \"wb\") as file:\n",
    "    pickle.dump(tuned_clf_svm, file)\n",
    "\n",
    "GetConfusionMatrix(tuned_clf_svm, x_train, x_test, y_train, y_test, label_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'metric': 'manhattan', 'n_neighbors': 15, 'weights': 'uniform'}\n",
      "Best cross-validation score: 0.628\n",
      "Classification TRAIN DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        12Hz       0.79      0.86      0.82        35\n",
      "         6Hz       1.00      0.41      0.59        29\n",
      "        24Hz       0.73      0.77      0.75        35\n",
      "        30Hz       0.68      0.91      0.78        35\n",
      "\n",
      "    accuracy                           0.75       134\n",
      "   macro avg       0.80      0.74      0.73       134\n",
      "weighted avg       0.79      0.75      0.74       134\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[30  0  2  3]\n",
      " [ 5 12  6  6]\n",
      " [ 2  0 27  6]\n",
      " [ 1  0  2 32]]\n",
      "Classification TEST DATA \n",
      "=======================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        12Hz       1.00      0.77      0.87        13\n",
      "         6Hz       1.00      0.11      0.19        19\n",
      "        24Hz       0.50      0.85      0.63        13\n",
      "        30Hz       0.46      0.85      0.59        13\n",
      "\n",
      "    accuracy                           0.59        58\n",
      "   macro avg       0.74      0.64      0.57        58\n",
      "weighted avg       0.77      0.59      0.53        58\n",
      "\n",
      "Confusion matrix \n",
      "=======================\n",
      "[[10  0  1  2]\n",
      " [ 0  2  8  9]\n",
      " [ 0  0 11  2]\n",
      " [ 0  0  2 11]]\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors= 10, weights = \"uniform\")\n",
    "\n",
    "param_grid = {\n",
    "    'n_neighbors': [1, 3, 5, 7, 9, 11, 13, 15],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan', 'chebyshev']\n",
    "}\n",
    "\n",
    "\n",
    "cv_splitter = KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "tuned_clf_knn = GridSearchCV(estimator=knn, param_grid=param_grid,\n",
    "                    scoring='accuracy', refit='accuracy', cv=cv_splitter)\n",
    "\n",
    "tuned_clf_knn.fit(x_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {tuned_clf_knn.best_params_}\")\n",
    "print(f\"Best cross-validation score: {tuned_clf_knn.best_score_:.3f}\")\n",
    "\n",
    "label_names = ['12Hz', '6Hz', '24Hz', '30Hz']\n",
    "\n",
    "with open(\"trained_model/KNN_model.pkl\", \"wb\") as file:\n",
    "    pickle.dump(tuned_clf_knn, file)\n",
    "\n",
    "GetConfusionMatrix(tuned_clf_knn, x_train, x_test, y_train, y_test, label_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
